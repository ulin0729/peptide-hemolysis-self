{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "number of test data: 277\n"
     ]
    }
   ],
   "source": [
    "def same_seed(seed): \n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def amino_encode_table_6(): # key: Amino Acid, value: tensor\n",
    "    df = pd.read_csv('./6-pc', sep=' ', index_col=0)\n",
    "    H1 = (df['H1'] - np.mean(df['H1'])) / (np.std(df['H1'], ddof=1))\n",
    "    V = (df['V'] - np.mean(df['V'])) / (np.std(df['V'], ddof=1))\n",
    "    P1 = (df['P1'] - np.mean(df['P1'])) / (np.std(df['P1'], ddof=1))\n",
    "    Pl = (df['Pl'] - np.mean(df['Pl'])) / (np.std(df['Pl'], ddof=1))\n",
    "    PKa = (df['PKa'] - np.mean(df['PKa'])) / (np.std(df['PKa'], ddof=1))\n",
    "    NCI = (df['NCI'] - np.mean(df['NCI'])) / (np.std(df['NCI'], ddof=1))\n",
    "    c = np.array([H1,V,P1,Pl,PKa,NCI], dtype=np.float32)\n",
    "    amino = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "    table = {}\n",
    "    for index,key in enumerate(amino):\n",
    "        # table[key] = torch.from_numpy(c[0:6, index])\n",
    "        table[key] = list(c[0:6, index])\n",
    "    table['X'] = [0,0,0,0,0,0]\n",
    "    return table\n",
    "\n",
    "table = amino_encode_table_6()\n",
    "\n",
    "def padding_seq(original_seq, length=50, pad_value='X'):\n",
    "    padded_seq = original_seq.ljust(length, pad_value)\n",
    "    return padded_seq\n",
    "\n",
    "def seq_to_features(seq):\n",
    "    features_list = []\n",
    "    for aa in seq:\n",
    "        features_list.append(table[aa])\n",
    "    feature_tensor = torch.Tensor(features_list)\n",
    "    return feature_tensor\n",
    "\n",
    "def seq_to_features_ml(seq, conc):\n",
    "    features_list = []\n",
    "    for aa in seq:\n",
    "        t = table[aa].copy()\n",
    "        t.append(conc)\n",
    "        features_list += t\n",
    "    feature_tensor = np.array(features_list, dtype=np.float32)\n",
    "    return feature_tensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "input_size = 6\n",
    "sequence_length = 50\n",
    "num_layers = 2\n",
    "\n",
    "hidden_size = 4\n",
    "num_epochs = 6000\n",
    "batch_size = 256\n",
    "bidirectional = True\n",
    "learning_rate = 1e-5\n",
    "early_stop = 1000\n",
    "weight_decay = 0.00001\n",
    "\n",
    "seed=10902128\n",
    "same_seed(seed)\n",
    "\n",
    "model_path = './class_model.ckpt'\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, bidirectional):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bi = 1 if bidirectional else 0\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional, dropout=0.3)\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if name.startswith('weight'):\n",
    "                nn.init.xavier_normal_(param)\n",
    "        self.act = nn.LeakyReLU(0.2)\n",
    "        self.fc = nn.Linear(hidden_size*(1+self.bi)+1, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, encoded, conc):\n",
    "        h0 = torch.zeros(self.num_layers*(1+self.bi), encoded.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(encoded, h0)\n",
    "        out = out[:, -1, :] # extract last time step\n",
    "        conc = torch.unsqueeze(conc, 1)\n",
    "        # out = self.act(out)\n",
    "        fc_in = torch.cat((out, conc), dim=1)\n",
    "        ret = self.fc(fc_in)\n",
    "        r = self.sig(ret)\n",
    "        return r\n",
    "\n",
    "class HemolysisDataset(Dataset):\n",
    "    def __init__(self, parquet):\n",
    "        df = pd.read_parquet(parquet)\n",
    "        df['sequence'] = df['sequence'].apply(padding_seq)\n",
    "        df['encoded'] = df['sequence'].apply(seq_to_features)\n",
    "        self.features = torch.stack(df['encoded'].values.tolist(), dim=0)\n",
    "        self.conc = torch.from_numpy(df['concentration'].values)\n",
    "        self.lysis = torch.from_numpy(df['lysis'].values)\n",
    "        self.n_samples = df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.conc[index], self.lysis[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "test_dataset = HemolysisDataset('./test.parquet')\n",
    "TestLoader = DataLoader(dataset=test_dataset, batch_size=10000000,shuffle=False, num_workers=0)\n",
    "print(f'number of test data: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./train.parquet')\n",
    "Y = df['label'].to_numpy(dtype=int)\n",
    "X = np.array([]).reshape(0,350)\n",
    "for i, row in df.iterrows():\n",
    "    X = np.vstack([X,seq_to_features_ml(padding_seq(row['sequence']), row['concentration'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet('./test.parquet')\n",
    "test_Y = test_df['label'].to_numpy(dtype=int)\n",
    "test_X = np.array([]).reshape(0,350)\n",
    "for i, row in test_df.iterrows():\n",
    "    test_X = np.vstack([test_X,seq_to_features_ml(padding_seq(row['sequence']), row['concentration'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([]).reshape(0,len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional).to(device)\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for i, (rnn_input, conc, labels) in enumerate(TestLoader):\n",
    "#         rnn_input = rnn_input.to(device)\n",
    "#         conc = conc.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(rnn_input, conc)\n",
    "#         outputs = outputs.squeeze(1)\n",
    "#         outputs = outputs.round()\n",
    "# rnn_pred = outputs.to('cpu').numpy().astype(int)\n",
    "# predictions = np.vstack([predictions, rnn_pred])\n",
    "\n",
    "# print(f'RNN acc: {accuracy_score(test_Y, rnn_pred)}, prec: {precision_score(test_Y, rnn_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc acc: 0.8086642599277978, prec: 0.8102189781021898\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc_clf = svm.SVC(random_state=seed)\n",
    "svc_clf.fit(X,Y)\n",
    "svc_pred = svc_clf.predict(test_X)\n",
    "predictions = np.vstack([predictions, svc_pred])\n",
    "print(f'svc acc: {accuracy_score(test_Y, svc_pred)}, prec: {precision_score(test_Y, svc_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf acc: 0.8303249097472925, prec: 0.8321167883211679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(random_state=seed)\n",
    "rf_clf.fit(X,Y)\n",
    "rf_pred = rf_clf.predict(test_X)\n",
    "predictions = np.vstack([predictions, rf_pred])\n",
    "print(f'rf acc: {accuracy_score(test_Y, rf_pred)}, prec: {precision_score(test_Y, rf_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# ada_clf = AdaBoostClassifier(random_state=seed)\n",
    "# ada_clf.fit(X,Y)\n",
    "# ada_pred = ada_clf.predict(test_X)\n",
    "# predictions = np.vstack([predictions, ada_pred])\n",
    "# print(f'ada acc: {accuracy_score(test_Y, ada_pred)}, prec: {precision_score(test_Y, ada_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# mlp_clf = MLPClassifier(max_iter=5000, random_state=seed)\n",
    "# mlp_clf.fit(X,Y)\n",
    "# mlp_pred = mlp_clf.predict(test_X)\n",
    "# predictions = np.vstack([predictions, mlp_pred])\n",
    "# print(f'mlp acc: {accuracy_score(test_Y, mlp_pred)}, prec: {precision_score(test_Y, mlp_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# knn_clf = KNeighborsClassifier()\n",
    "# knn_clf.fit(X,Y)\n",
    "# knn_pred = knn_clf.predict(test_X)\n",
    "# predictions = np.vstack([predictions, knn_pred])\n",
    "# print(f'knn acc: {accuracy_score(test_Y, knn_pred)}, prec: {precision_score(test_Y, knn_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb acc: 0.8231046931407943, prec: 0.8345864661654135\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_clf = xgb.XGBClassifier(random_state=seed)\n",
    "xgb_clf.fit(X,Y)\n",
    "xgb_pred = xgb_clf.predict(test_X)\n",
    "predictions = np.vstack([predictions, xgb_pred])\n",
    "print(f'xgb acc: {accuracy_score(test_Y, xgb_pred)}, prec: {precision_score(test_Y, xgb_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 277)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = predictions.sum(axis=0)\n",
    "predict[predict <= (len(predictions)//2)] = 0\n",
    "predict[predict > (len(predictions)//2)] = 1\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final acc: 0.8375451263537906, prec: 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "final_pred = predict\n",
    "print(f'final acc: {accuracy_score(test_Y, final_pred)}, prec: {precision_score(test_Y, final_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold=20: final acc: 0.8375451263537906,  prec: 0.8444444444444444    (All without knn, rnn, mlp, ada)\n",
    "# threshold=30: final acc: 0.8125,              prec: 0.8125                (All without knn, rnn, mlp, ada)\n",
    "# threshold=40: final acc: 0.8,                 prec: 0.8288288288288288    (All without knn, rnn, ada)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c92b8c09fadef409cad5c4ed1ba21a2a5da6ac080dda7f85b536f769f0caa6dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
